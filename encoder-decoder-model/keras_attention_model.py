# -*- coding: utf-8 -*-
"""keras_Attention_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dLJX4_E6ZHyOTs7HA97f6QRL8yj1GaAJ
"""

import tensorflow as tf
from keras.layers import Layer
import keras.backend as K
import numpy as np
import matplotlib.pyplot as plt


from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply , Embedding , Dense
from keras.layers import RepeatVector, Dense, Activation, Lambda , LSTMCell , Reshape , Lambda
from keras.optimizers import Adam
from keras.utils import to_categorical
from keras.models import load_model, Model

class attention(Layer):
    def __init__(self,**kwargs):
        super(attention,self).__init__(**kwargs)

    def build(self,input_shape):
        self.W=self.add_weight(name="att_weight",shape=(input_shape[-1],1),initializer="normal")
        self.b=self.add_weight(name="att_bias",shape=(input_shape[1],1),initializer="zeros")        
        super(attention, self).build(input_shape)

    def call(self,x):
        et=K.squeeze(K.relu(K.dot(x,self.W)+self.b),axis=-1)
        at=K.softmax(et)
        at=K.expand_dims(at,axis=-1)
        output=x*at
        return K.sum(output,axis=1)

    def compute_output_shape(self,input_shape):
        return (input_shape[0],input_shape[-1])

    def get_config(self):
        return super(attention,self).get_config()

class seq2seqmodel():

  def __init__(self, Tx, Ty, rnn_hidden_units, embedding_dims, 
                 vocab_size, embedding_matrix):
      
    self.Tx = Tx
    self.Ty = Ty
    self.rnn_hidden_units = rnn_hidden_units
    self.embedding_dims = embedding_dims
    self.vocab_size = vocab_size
    self.embedding_matrix = embedding_matrix
    self.batch_size = batch_size
    self.attention_layer = attention()
    self.densor1 = Dense(10 , activation = 'tanh')
    self.densor2 = Dense(1 , activation = 'relu')
    self.post_lstm_layer = LSTM(self.rnn_hidden_units, return_state = True)
    self.repeator = RepeatVector(self.Tx)
    self.concatenator = Concatenate(axis=-1)
    self.ouput_layer = Dense(self.vocab_size , activation = 'softmax')

  def build(self,
            input_shape = None,
            output_shape = None):
    self.input_shape = input_shape
    self.ouput_shape = output_shape


    encoder_input = Input(shape = (self.Tx,) , name = "encoder_input")
    decoder_input = Input(shape = (self.Ty,) , name = "decoder_input")
    S0 = Input(shape = (self.rnn_hidden_units ,) , name ="S0")
    C0 = Input(shape = (self.rnn_hidden_units ,) , name = "C0")

    encoder_embeddings = Embedding(input_dim = self.vocab_size+1,
                                  output_dim = self.embedding_dims,
                                  weights = [self.embedding_matrix], 
                                  trainable = False)(encoder_input)
    
    decoder_embeddings = Embedding(input_dim = self.vocab_size+1,
                                  output_dim = self.embedding_dims,
                                  weights = [self.embedding_matrix], 
                                  trainable = False)(decoder_input)
    
    activations,last_hidden_state,last_cell_state = (LSTM(units = self.rnn_hidden_units,
                                                                return_sequences = True,
                                                                return_state = True))(encoder_embeddings)

    s = S0
    c = last_cell_state
    outputs = []
    
    for t in range(self.Ty):

      repeator = self.repeator(s)
      
      concat = self.concatenator(inputs = [activations, repeator])
      
      context = self.attention_layer(inputs = concat)

      d_embeddings = Lambda(lambda x: x[:,t,:])(decoder_embeddings)

      decoder = self.concatenator(inputs = [context , d_embeddings ])
      
      decoder = Reshape(target_shape = (1 , 712))(decoder)
      
      s, _ , c = self.post_lstm_layer(inputs = decoder , initial_state = [s ,c])

      out = self.ouput_layer(s)

      outputs.append(out)

    
    model = Model(inputs = [encoder_input , decoder_input , S0 , C0] , outputs = out )

    return model

Tx = 15
Ty = 15
rnn_hidden_units = 256
embedding_dims = 200
vocab_size = 1000
embedding_matrix = np.zeros((1001,200))
dense_units = 256
batch_size = 500
attention_layer = attention()
batch_size = 100

model = seq2seqmodel(Tx, Ty, rnn_hidden_units, embedding_dims, 
                 vocab_size, embedding_matrix)

model =model.build()
model.summary()



