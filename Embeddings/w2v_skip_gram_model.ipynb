{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v_skip_gram_model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDMrxVE9HvpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word2vec skip-gram model implementation using numpy\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "window_size = 2\n",
        "n= 16\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "\n",
        "class word2vec():\n",
        "  def __init__(self):\n",
        "    self.n = n\n",
        "    self.lr = learning_rate\n",
        "    self.epochs = epochs\n",
        "    self.window = window_size\n",
        "\n",
        "  def generate_training_data(self,corpus):\n",
        "    word_counts = defaultdict(int)\n",
        "    for row in corpus:\n",
        "      for word in row:\n",
        "        word_counts[word] += 1\n",
        "    \n",
        "    self.v_count = len(word_counts.keys())\n",
        "    self.word_list = list(word_counts.keys())\n",
        "    self.word_to_index = dict((word , i) for i,word in enumerate(self.word_list))\n",
        "    self.index_to_word = dict((i , word) for i,word in enumerate(self.word_list))\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for sentence in corpus:\n",
        "      sent_len = len(sentence)\n",
        "      for i , word in enumerate(sentence):\n",
        "        w_target = self.word2onehot(sentence[i])\n",
        "        w_context = []\n",
        "\n",
        "        for j in range(i-self.window ,i+self.window +1):\n",
        "          \n",
        "          if j!= i and j <= sent_len - 1 and j>=0:\n",
        "            w_context.append(self.word2onehot(sentence[j]))\n",
        "            training_data.append([w_target,w_context])\n",
        "  \n",
        "    return np.array(training_data)\n",
        "\n",
        "  def word2onehot(self,word):\n",
        "    word_vec = np.zeros((self.v_count,1))\n",
        "    word_index = self.word_to_index[word]\n",
        "    word_vec[word_index] = 1\n",
        "\n",
        "    return word_vec\n",
        "\n",
        "  def normalized_softmax(self,x):\n",
        "    e_x = np.exp(x-np.max(x))\n",
        "    return e_x/e_x.sum()\n",
        "\n",
        "  def forward_prop(self,x):\n",
        "\n",
        "    # ref http://alexminnaar.com/2015/04/12/word2vec-tutorial-skipgram.html\n",
        "\n",
        "    h = np.dot(self.w1.T ,x)\n",
        "    u = np.dot(self.w2.T, h)\n",
        "    y = self.normalized_softmax(u)\n",
        "\n",
        "    return y ,h,u\n",
        "\n",
        "  def backward_prop(self, e ,h ,x):\n",
        "    \n",
        "    # ref http://www.claudiobellei.com/2018/01/06/backprop-word2vec/\n",
        "\n",
        "    dw2 = np.outer(h,e)\n",
        "    dw1 = np.outer(x, np.dot(self.w2 ,e.T))\n",
        "\n",
        "    alpha = self.lr\n",
        "\n",
        "    self.w1 = self.w1 - alpha*dw1\n",
        "    self.w2 = self.w2 - alpha*dw2\n",
        "\n",
        "  def train(self, training_data):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    self.w1 = np.random.randn(self.v_count , self.n) * 0.01\n",
        "    self.w2 = np.random.randn(self.n, self.v_count) * 0.01\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "      \n",
        "      self.loss = 0\n",
        "\n",
        "      for w_t ,w_c in training_data:\n",
        "\n",
        "        y_pred , h , u = self.forward_prop(w_t)\n",
        "\n",
        "        error_target= np.sum([np.subtract(y_pred,word) for word in w_c] , axis =0)\n",
        "\n",
        "        self.backward_prop(error_target, h ,w_t)\n",
        "\n",
        "        self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
        "      \n",
        "      print('Epoch:', i, \"Loss:\", self.loss)\n",
        "        \n",
        "\n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAw4hkKBH1Ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}